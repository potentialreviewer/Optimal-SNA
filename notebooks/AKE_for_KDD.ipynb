{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!pip install datasets\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "GK8X_WnWvl_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "benchmark = \"KDD\"\n",
        "dataset = load_dataset(\n",
        "    \"parquet\",\n",
        "    data_files={\n",
        "        \"test\":  \"https://huggingface.co/datasets/midas/kdd/resolve/refs/convert/parquet/raw/test/0000.parquet\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "AmyRF013vn13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in dataset.keys():\n",
        "    dataset[split] = dataset[split].map(lambda x: {'keyphrases': x['extractive_keyphrases'] + x['abstractive_keyphrases']})"
      ],
      "metadata": {
        "id": "5gF7C0d7vq7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_parentheses_before_detokenize(tokens):\n",
        "    return [\n",
        "        token.replace('-LRB-', '(')\n",
        "             .replace('-RRB-', ')')\n",
        "             .replace('-LSB-', '[')\n",
        "             .replace('-RSB-', ']')\n",
        "             .replace('-LCB-', '{')\n",
        "             .replace('-RCB-', '}')\n",
        "             .replace('-LT-', '<')\n",
        "             .replace('-GT-', '>')\n",
        "        for token in tokens\n",
        "    ]"
      ],
      "metadata": {
        "id": "IeLb_rYYm0Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in dataset.keys():\n",
        "    dataset[split] = dataset[split].map(lambda x: {\n",
        "        'document': replace_parentheses_before_detokenize(list(x['document']))\n",
        "    })"
      ],
      "metadata": {
        "id": "FYyaq_HCvs-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import html\n",
        "\n",
        "html_char_map = {name: chr(code) for name, code in html.entities.name2codepoint.items()}\n",
        "html_codepoint_map = {code: chr(code) for code in html.entities.codepoint2name}"
      ],
      "metadata": {
        "id": "LXlTeAqDow-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_html_special_characters(tokens):\n",
        "    result = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if i + 2 < len(tokens) and tokens[i] == '&' and tokens[i + 1] in html_char_map and tokens[i + 2] == ';':\n",
        "            result.append(html_char_map[tokens[i + 1]])\n",
        "            i += 3\n",
        "        elif i < len(tokens) and tokens[i].startswith('&#') and tokens[i].endswith(';'):\n",
        "            codepoint = int(tokens[i][2:-1])\n",
        "            result.append(chr(codepoint))\n",
        "            i += 1\n",
        "        else:\n",
        "            result.append(tokens[i])\n",
        "            i += 1\n",
        "    return result"
      ],
      "metadata": {
        "id": "iNb1aeqMoxBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in dataset.keys():\n",
        "    dataset[split] = dataset[split].map(lambda x: {\n",
        "        'document': replace_html_special_characters(list(x['document']))\n",
        "    })"
      ],
      "metadata": {
        "id": "W4qd2LVOvu53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "for split in dataset.keys():\n",
        "    dataset[split] = dataset[split].map(lambda x: {\n",
        "        'document': tokenizer.convert_tokens_to_string(list(x['document']))\n",
        "    })"
      ],
      "metadata": {
        "id": "-P9FuTuGvzHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_after_detokenize(text):\n",
        "    text = text.replace('( ', '(')\n",
        "    text = text.replace(' )', ')')\n",
        "    text = text.replace('[ ', '[')\n",
        "    text = text.replace(' ]', ']')\n",
        "    text = text.replace('{ ', '{')\n",
        "    text = text.replace(' }', '}')\n",
        "    text = text.replace(' :', ':')\n",
        "    text = text.replace(' ;', ';')\n",
        "    return text"
      ],
      "metadata": {
        "id": "UO8SeCplo6MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in dataset.keys():\n",
        "    dataset[split] = dataset[split].map(lambda x: {\n",
        "        'document': clean_text_after_detokenize(x['document'])\n",
        "    })"
      ],
      "metadata": {
        "id": "RqOHFmJEv06D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_empty_entries_from_dataset(data):\n",
        "    for split in data.keys():\n",
        "        original_count = len(data[split])\n",
        "\n",
        "        data[split] = data[split].filter(\n",
        "            lambda x: (\n",
        "                x['id'] and\n",
        "                x['document'] and\n",
        "                x['keyphrases']\n",
        "            )\n",
        "        )\n",
        "\n",
        "        cleaned_count = len(data[split])\n",
        "\n",
        "        print(f\"[{split}] Before data cleaning: {original_count}\")\n",
        "        print(f\"[{split}] After data cleaning: {cleaned_count}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "dataset = remove_empty_entries_from_dataset(dataset)"
      ],
      "metadata": {
        "id": "HllvAXHtv21W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "5yVWaZ2lv_lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** Unless otherwise noted in this notebook, the implementations are based on the [Python-based Keyphrase Extraction toolkit (PKE)](https://github.com/boudinfl/pke) by [Boudin (2016)](https://aclanthology.org/C16-2015/), tailored to fit our work."
      ],
      "metadata": {
        "id": "e_DReT0KLAty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokenizer import _get_regex_pattern\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
        "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
        "from spacy.util import compile_infix_regex\n",
        "\n",
        "infixes = (\n",
        "    LIST_ELLIPSES\n",
        "    + LIST_ICONS\n",
        "    + [\n",
        "        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
        "        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
        "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
        "        ),\n",
        "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
        "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
        "    ]\n",
        ")\n",
        "\n",
        "infix_re = compile_infix_regex(infixes)\n",
        "nlp.tokenizer.infix_finditer = infix_re.finditer"
      ],
      "metadata": {
        "id": "zmvJ0QXVpONX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer as Stemmer\n",
        "\n",
        "all_data = []\n",
        "references = []\n",
        "stemmer = Stemmer('porter')\n",
        "\n",
        "for split in dataset.keys():\n",
        "    for sample in tqdm(dataset[split]):\n",
        "        all_data.append(nlp(sample[\"document\"]))\n",
        "\n",
        "        sample_keyphrases = []\n",
        "        for keyphrase in sample[\"keyphrases\"]:\n",
        "            tokens = [token.text for token in nlp(keyphrase)]\n",
        "            stems = [stemmer.stem(tok.lower()) for tok in tokens]\n",
        "            sample_keyphrases.append(\" \".join(stems))\n",
        "        references.append(sample_keyphrases)"
      ],
      "metadata": {
        "id": "VERz8xRMwB1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pke import compute_document_frequency\n",
        "from string import punctuation\n",
        "from pke import load_document_frequency_file"
      ],
      "metadata": {
        "id": "Lgkmo47uqFD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_document_frequency(\n",
        "    documents=all_data,\n",
        "    output_file='data/{}.df.gz'.format(benchmark),\n",
        "    language='en',\n",
        "    normalization='stemming',\n",
        "    stoplist=list(punctuation),\n",
        "    n=5\n",
        ")\n",
        "\n",
        "df = load_document_frequency_file(input_file='data/{}.df.gz'.format(benchmark))"
      ],
      "metadata": {
        "id": "iAzo14M5piaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import logging\n",
        "\n",
        "from pke.base import LoadFile"
      ],
      "metadata": {
        "id": "EPAg8ENdAZcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TF\n",
        "\n",
        "class TF(LoadFile):\n",
        "\n",
        "      def candidate_selection(self, n=3):\n",
        "          self.ngram_selection(n=n)\n",
        "          self.candidate_filtering()\n",
        "\n",
        "      def candidate_weighting(self):\n",
        "        for k, v in self.candidates.items():\n",
        "            self.weights[k] = len(v.surface_forms)\n",
        "            self.weights[k] += (self.candidates[k].offsets[0] * 1e-8)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sxYFEUEsuTDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx"
      ],
      "metadata": {
        "id": "1KTjotqHAbp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TextRank\n",
        "\n",
        "class TextRank1(LoadFile):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TextRank1, self).__init__()\n",
        "        self.graph = nx.Graph()\n",
        "\n",
        "    def candidate_selection(self, pos=None):\n",
        "\n",
        "        if pos is None:\n",
        "            pos = {'NOUN', 'PROPN', 'ADJ'}\n",
        "\n",
        "        self.longest_pos_sequence_selection(valid_pos=pos)\n",
        "\n",
        "    def build_word_graph(self, window=2, pos=None):\n",
        "\n",
        "        if pos is None:\n",
        "            pos = {'NOUN', 'PROPN', 'ADJ'}\n",
        "\n",
        "        text = [(word, sentence.pos[i] in pos) for sentence in self.sentences\n",
        "                for i, word in enumerate(sentence.stems)]\n",
        "\n",
        "        self.graph.add_nodes_from([word for word, valid in text if valid])\n",
        "\n",
        "        for i, (node1, is_in_graph1) in enumerate(text):\n",
        "\n",
        "            if not is_in_graph1:\n",
        "                continue\n",
        "\n",
        "            for j in range(i + 1, min(i + window, len(text))):\n",
        "                node2, is_in_graph2 = text[j]\n",
        "                if is_in_graph2 and node1 != node2:\n",
        "                    self.graph.add_edge(node1, node2)\n",
        "\n",
        "    def candidate_weighting(self,\n",
        "                            window=2,\n",
        "                            pos=None,\n",
        "                            top_percent=None,\n",
        "                            normalized=False):\n",
        "\n",
        "        if pos is None:\n",
        "            pos = {'NOUN', 'PROPN', 'ADJ'}\n",
        "\n",
        "        self.build_word_graph(window=window, pos=pos)\n",
        "\n",
        "        w = nx.pagerank(self.graph, alpha=0.85, tol=0.0001, weight=None)\n",
        "\n",
        "        if top_percent is not None:\n",
        "\n",
        "            nb_nodes = self.graph.number_of_nodes()\n",
        "            to_keep = min(math.floor(nb_nodes * top_percent), nb_nodes)\n",
        "\n",
        "            top_words = sorted(w, key=w.get, reverse=True)\n",
        "\n",
        "            self.longest_keyword_sequence_selection(top_words[:int(to_keep)])\n",
        "\n",
        "        for k in self.candidates.keys():\n",
        "            tokens = self.candidates[k].lexical_form\n",
        "            self.weights[k] = sum([w[t] for t in tokens])\n",
        "            if normalized:\n",
        "                self.weights[k] /= len(tokens)\n",
        "\n",
        "            self.weights[k] += (self.candidates[k].offsets[0]*1e-8)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VkSBBmCyZ_qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** The implementation of KeyBERT [(Grootendorst, 2020)](https://github.com/MaartenGr/KeyBERT) is based on [this](https://github.com/MaartenGr/KeyBERT) and tailored to fit our work."
      ],
      "metadata": {
        "id": "KFgKLtW0Lv5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "\n",
        "from collections import Counter\n",
        "from keybert.backend._utils import select_backend\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "KeyBERT_model = select_backend(\"all-distilroberta-v1\")"
      ],
      "metadata": {
        "id": "Le5Fr3ppwKxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title KeyBERT\n",
        "\n",
        "class KeyBERT(LoadFile):\n",
        "    def __init__(self, model=KeyBERT_model):\n",
        "        super(KeyBERT, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def candidate_weighting(self, input):\n",
        "        doc = str(input)\n",
        "\n",
        "        words = []\n",
        "\n",
        "        for key, candidate in self.candidates.items():\n",
        "            combined_surface_forms = [' '.join(form) for form in candidate.surface_forms]\n",
        "            form_counter = Counter(combined_surface_forms)\n",
        "            most_common_forms = form_counter.most_common()\n",
        "            max_frequency = most_common_forms[0][1]\n",
        "            candidates_with_max_freq = [form for form, freq in most_common_forms if freq == max_frequency]\n",
        "            sorted_combined_surface_forms = sorted(zip(candidate.offsets, combined_surface_forms))\n",
        "            for offset, form in sorted_combined_surface_forms:\n",
        "                if form in candidates_with_max_freq:\n",
        "                    words.append(str(form))\n",
        "                    break\n",
        "\n",
        "        doc_embedding = self.model.embed(doc).reshape(1, -1)\n",
        "        word_embeddings = self.model.embed(words)\n",
        "        similarities = cosine_similarity(doc_embedding, word_embeddings)[0]\n",
        "        self.weights = {word: round(float(sim), 4) for word, sim in zip(words, similarities)}\n",
        "        del doc_embedding\n",
        "        del word_embeddings\n",
        "        del similarities\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "32DC9T6orJ10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** The implementation of MDERank [(Zhang et al., 2022)](https://aclanthology.org/2022.findings-acl.34/) is based on [this](https://github.com/LinhanZ/mderank) and tailored to fit our work."
      ],
      "metadata": {
        "id": "a_MV8suSLIY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip\"\n",
        "file_name = \"/content/stanford-corenlp-full-2018-02-27.zip\"\n",
        "\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "with zipfile.ZipFile(file_name, 'r') as zip_path:\n",
        "    zip_path.extractall(\"/content/\")"
      ],
      "metadata": {
        "id": "K1v5GXUSq5Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanfordcorenlp"
      ],
      "metadata": {
        "id": "4ZTGf7mcwMhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from accelerate import Accelerator\n",
        "from transformers import BertForMaskedLM, BertTokenizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "\n",
        "GRAMMAR1 = \"\"\"  NP:\n",
        "        {<NN.*|JJ>*<NN.*>}\"\"\"\n",
        "\n",
        "stopword_dict = set(nltk_stopwords.words('english'))\n",
        "wnl=nltk.WordNetLemmatizer()\n",
        "\n",
        "en_model = StanfordCoreNLP(r'/content/stanford-corenlp-full-2018-02-27',quiet=True)\n",
        "\n",
        "MDERank_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "MDERank_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MDERank_model.to(device)\n",
        "\n",
        "MAX_LEN =512"
      ],
      "metadata": {
        "id": "diWP0wvuwO4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MDERank\n",
        "\n",
        "class MDERANK():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.tokens = []\n",
        "        self.tokens_tagged = []\n",
        "        self.keyphrase_candidate = []\n",
        "\n",
        "    @staticmethod\n",
        "    def remove(text):\n",
        "        text_len = len(text.split())\n",
        "        remove_chars = '[’!\"#$%&\\'()*+,./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
        "        text = re.sub(remove_chars, '', text)\n",
        "        re_text_len = len(text.split())\n",
        "        if text_len != re_text_len:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def extract_candidates(self, tokens_tagged, no_subset=False):\n",
        "        np_parser = nltk.RegexpParser(GRAMMAR1)\n",
        "        keyphrase_candidate = []\n",
        "        np_pos_tag_tokens = np_parser.parse(tokens_tagged)\n",
        "        count = 0\n",
        "        for token in np_pos_tag_tokens:\n",
        "            if (isinstance(token, nltk.tree.Tree) and token._label == \"NP\"):\n",
        "                np = ' '.join(word for word, tag in token.leaves())\n",
        "                length = len(token.leaves())\n",
        "                start_end = (count, count + length)\n",
        "                count += length\n",
        "                keyphrase_candidate.append((np, start_end))\n",
        "\n",
        "            else:\n",
        "                count += 1\n",
        "\n",
        "        return keyphrase_candidate\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_absent_doc(ori_encode_dict, candidates):\n",
        "\n",
        "        count = 0\n",
        "        doc_pairs = []\n",
        "        ori_input_ids = ori_encode_dict[\"input_ids\"].squeeze()\n",
        "        ori_tokens = MDERank_tokenizer.convert_ids_to_tokens(ori_input_ids)\n",
        "\n",
        "        for id, candidate in enumerate(candidates):\n",
        "            if MDERANK.remove(candidate):\n",
        "                count +=1\n",
        "                continue\n",
        "\n",
        "            tok_candidate = MDERank_tokenizer.tokenize(candidate)\n",
        "            candidate_len = len(tok_candidate)\n",
        "            mask = ' '.join(['[MASK]'] * candidate_len)\n",
        "            ori_doc = ' '.join(ori_tokens)\n",
        "            can_token = ' '.join(tok_candidate)\n",
        "\n",
        "            try:\n",
        "                candidate_re = re.compile(r\"\\b\" + can_token + r\"\\b\")\n",
        "                masked_doc = re.sub(candidate_re, mask, ori_doc)\n",
        "                match = candidate_re.findall(ori_doc)\n",
        "            except:\n",
        "                count +=1\n",
        "                continue\n",
        "            if len(match) == 0:\n",
        "                count +=1\n",
        "                continue\n",
        "\n",
        "            masked_tokens = masked_doc.split()\n",
        "            masked_input_ids = MDERank_tokenizer.convert_tokens_to_ids(masked_tokens)\n",
        "            len_masked_tokens = len(masked_tokens) - masked_tokens.count('[PAD]')\n",
        "\n",
        "            try:\n",
        "                assert len(masked_input_ids) == 512\n",
        "            except:\n",
        "                count +=1\n",
        "                continue\n",
        "\n",
        "            masked_attention_mask = np.zeros(MAX_LEN)\n",
        "            masked_attention_mask[:len_masked_tokens] = 1\n",
        "            masked_token_type_ids = np.zeros(MAX_LEN)\n",
        "            masked_encode_dict = {\n",
        "                \"input_ids\": torch.Tensor(masked_input_ids).to(torch.long),\n",
        "                \"token_type_ids\": torch.Tensor(masked_token_type_ids).to(torch.long),\n",
        "                \"attention_mask\": torch.Tensor(masked_attention_mask).to(torch.long),\n",
        "                \"candidate\": candidate,\n",
        "                \"freq\": len(match)\n",
        "            }\n",
        "            doc_pairs.append([ori_encode_dict, masked_encode_dict])\n",
        "\n",
        "        return doc_pairs, count\n",
        "\n",
        "    def mean_pooling(model_output, attention_mask):\n",
        "        hidden_states = model_output.hidden_states\n",
        "        token_embeddings = hidden_states[-1]\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_all_dist(dist_list):\n",
        "        dist_all={}\n",
        "        for pair in dist_list:\n",
        "            phrase = pair[0]\n",
        "            phrase = phrase.lower()\n",
        "            phrase = wnl.lemmatize(phrase)\n",
        "            if(phrase in dist_all):\n",
        "                dist_all[phrase].append(pair[1])\n",
        "            else:\n",
        "                dist_all[phrase]=[]\n",
        "                dist_all[phrase].append(pair[1])\n",
        "        return dist_all\n",
        "\n",
        "    @staticmethod\n",
        "    def get_final_dist(dist_all, method=\"average\"):\n",
        "        final_dist={}\n",
        "\n",
        "        if(method==\"average\"):\n",
        "\n",
        "            for phrase, dist_list in dist_all.items():\n",
        "                sum_dist = 0.0\n",
        "                for dist in dist_list:\n",
        "                    sum_dist += dist\n",
        "                if (phrase in stopword_dict):\n",
        "                    sum_dist = 0.0\n",
        "                final_dist[phrase] = sum_dist/float(len(dist_list))\n",
        "            return final_dist\n",
        "\n",
        "    def MDERank(self, text):\n",
        "        self.tokens = en_model.word_tokenize(text)\n",
        "        self.tokens_tagged = en_model.pos_tag(text)\n",
        "        assert len(self.tokens) == len(self.tokens_tagged)\n",
        "        for i, token in enumerate(self.tokens):\n",
        "            if token.lower() in stopword_dict:\n",
        "                self.tokens_tagged[i] = (token, \"IN\")\n",
        "        self.keyphrase_candidate = self.extract_candidates(self.tokens_tagged)\n",
        "\n",
        "        cans = self.keyphrase_candidate\n",
        "        candidates = []\n",
        "        for can, pos in cans:\n",
        "            candidates.append(can.lower())\n",
        "\n",
        "        ori_encode_dict = MDERank_tokenizer.encode_plus(\n",
        "            doc,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        doc_pairs, count = MDERANK.generate_absent_doc(ori_encode_dict, candidates)\n",
        "\n",
        "        MDERank_model.eval()\n",
        "\n",
        "        dist_list = []\n",
        "\n",
        "        for doc_pair in doc_pairs:\n",
        "            ori_input_ids = doc_pair[0][\"input_ids\"].squeeze(0).to(device)\n",
        "            ori_token_type_ids = doc_pair[0][\"token_type_ids\"].squeeze(0).to(device)\n",
        "            ori_attention_mask = doc_pair[0][\"attention_mask\"].squeeze(0).to(device)\n",
        "\n",
        "            masked_input_ids = doc_pair[1][\"input_ids\"].squeeze(0).to(device)\n",
        "            masked_token_type_ids = doc_pair[1][\"token_type_ids\"].squeeze(0).to(device)\n",
        "            masked_attention_mask = doc_pair[1][\"attention_mask\"].squeeze(0).to(device)\n",
        "            candidate = doc_pair[1][\"candidate\"]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                ori_outputs = MDERank_model(input_ids=ori_input_ids.unsqueeze(0), attention_mask=ori_attention_mask.unsqueeze(0), token_type_ids=ori_token_type_ids.unsqueeze(0), output_hidden_states=True)\n",
        "                masked_outputs = MDERank_model(input_ids=masked_input_ids.unsqueeze(0), attention_mask=masked_attention_mask.unsqueeze(0), token_type_ids=masked_token_type_ids.unsqueeze(0), output_hidden_states=True)\n",
        "\n",
        "                ori_doc_embed = MDERANK.mean_pooling(ori_outputs, ori_attention_mask)\n",
        "                masked_doc_embed = MDERANK.mean_pooling(masked_outputs, masked_attention_mask)\n",
        "\n",
        "            cosine_similarity = torch.cosine_similarity(ori_doc_embed, masked_doc_embed, dim=1).cpu()\n",
        "            score = cosine_similarity.item()\n",
        "\n",
        "            dist_list.append((candidate, score))\n",
        "\n",
        "            dist_all = MDERANK.get_all_dist(dist_list)\n",
        "            dist_final = MDERANK.get_final_dist(dist_all, method='average')\n",
        "\n",
        "            seen_stems = {}\n",
        "            for phrase, score in dist_final.items():\n",
        "                tokens = [token.text for token in nlp(phrase)]\n",
        "                stems = [stemmer.stem(tok.lower()) for tok in tokens]\n",
        "                stemmed_phrase = \" \".join(stems)\n",
        "\n",
        "                if stemmed_phrase not in seen_stems:\n",
        "                    seen_stems[stemmed_phrase] = score\n",
        "                elif score < seen_stems[stemmed_phrase]:\n",
        "                    seen_stems[stemmed_phrase] = score\n",
        "\n",
        "            dist_sorted = sorted(seen_stems.items(), key=lambda x: x[1], reverse=False)\n",
        "        return dist_sorted"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FlDXM_U8q5dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** The implementation of LMRank [(Giarelies and Karacapilidis, 2023)](https://ieeexplore.ieee.org/document/10179894) is based on [this](https://github.com/NC0DER/LMRank/blob/main/LMRank/model.py) and tailored to fit our work."
      ],
      "metadata": {
        "id": "G4t4TYQELDek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "UerabEikwSnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "import numpy.typing\n",
        "import faiss\n",
        "\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "from difflib import get_close_matches\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import TypeVar, List, Tuple, Any\n",
        "\n",
        "Model = TypeVar('Model')"
      ],
      "metadata": {
        "id": "pGdvAIeLApSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LMRank\n",
        "\n",
        "class LMRANK():\n",
        "    def __init__(self: LMRANK) -> None:\n",
        "\n",
        "        self.model = SentenceTransformer('all-mpnet-base-v2')\n",
        "        self.text = None\n",
        "        self.doc = None\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_last_seps(string: str, seps: str = '!?.') -> str:\n",
        "        sep_set = set(seps)\n",
        "        for i in range(len(string) - 1, -1, -1):\n",
        "            if string[i - 1] in sep_set:\n",
        "                return string[:i - 1]\n",
        "        return string\n",
        "\n",
        "    @staticmethod\n",
        "    def find_nth_occurence(string: str, substring: str, start: int, end: int, n: int) -> int:\n",
        "\n",
        "        i = string.find(substring, start, end)\n",
        "        while i >= 0 and n > 1:\n",
        "            i = string.find(substring, i + len(substring))\n",
        "            n -= 1\n",
        "        return i\n",
        "\n",
        "    @staticmethod\n",
        "    def create_chunks(string: str, max_token_length: int, token_sep: str = ' ') -> List[str]:\n",
        "\n",
        "        chunk_ranges = []\n",
        "        chunk_start = 0\n",
        "        chunk_end = 0\n",
        "\n",
        "        while chunk_end < len(string):\n",
        "\n",
        "            chunk_start = chunk_end\n",
        "\n",
        "            next_sep_pos = LMRANK.find_nth_occurence(\n",
        "                string, token_sep, chunk_start, len(string),\n",
        "                max_token_length\n",
        "            )\n",
        "\n",
        "            if next_sep_pos == -1:\n",
        "                chunk_end = len(string)\n",
        "            else:\n",
        "                chunk_end = next_sep_pos\n",
        "\n",
        "            chunk_ranges.append((chunk_start, chunk_end))\n",
        "\n",
        "        chunks = [string[i:j] for (i,j) in chunk_ranges]\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def extract_candidate_keyphrases(\n",
        "            self: LMRANK, text: str, sentence_seps: str = '!?.',\n",
        "            deduplicate: bool = True, keep_nouns_adjs: bool = True,\n",
        "        ) -> List[Tuple[str, int]]:\n",
        "\n",
        "        self.text = ' '.join(text.split())\n",
        "\n",
        "        self.doc = nlp(self.text)\n",
        "\n",
        "        candidate_keyphrases = [\n",
        "            (LMRANK.remove_last_seps(chunk.text.lower(), sentence_seps), chunk.start)\n",
        "            for chunk in self.doc.noun_chunks\n",
        "            if chunk.text.lower() not in nlp.Defaults.stop_words\n",
        "            and chunk[0].pos_ not in {'PRON', 'PART'}\n",
        "            and all(\n",
        "                term.pos_ in {'NOUN', 'ADJ'}\n",
        "                if keep_nouns_adjs else True for term in chunk\n",
        "            )\n",
        "            and len(chunk.text) > 2\n",
        "            and not chunk.text[:1].isdigit()\n",
        "            and not any(term.like_url or term.like_email for term in chunk)\n",
        "        ]\n",
        "\n",
        "        candidate_keyphrases = {\n",
        "            key: next(group)[1]\n",
        "            for key, group in groupby(\n",
        "                sorted(candidate_keyphrases, key = itemgetter(0)),\n",
        "                itemgetter(0))\n",
        "        }\n",
        "\n",
        "        if deduplicate:\n",
        "            string_similarity = 0.65\n",
        "\n",
        "            for item in list(candidate_keyphrases):\n",
        "                close_matches = get_close_matches(item, candidate_keyphrases.keys(),\n",
        "                                                  cutoff = 0.65, n = 10)[1:]\n",
        "                for close_match in close_matches:\n",
        "                    if not item.count(' '):\n",
        "                        candidate_keyphrases.pop(item, None)\n",
        "                        break\n",
        "                    elif (len(close_match) > len(item)\n",
        "                            and len(get_close_matches(item, [close_match], n = 1, cutoff = string_similarity))):\n",
        "                        candidate_keyphrases.pop(close_match, None)\n",
        "\n",
        "        return list(candidate_keyphrases.items())\n",
        "\n",
        "    def encode(\n",
        "            self: LMRANK, string_list: List[str],\n",
        "            multi_processing: bool = False, device: str = 'cuda'\n",
        "        ) -> numpy.typing.NDArray[Any]:\n",
        "\n",
        "        model = self.model\n",
        "\n",
        "        if multi_processing:\n",
        "            pool = model.start_multi_process_pool(target_devices = [device])\n",
        "            embeddings = model.encode_multi_process(string_list, pool)\n",
        "            model.stop_multi_process_pool(pool)\n",
        "        else:\n",
        "            embeddings = model.encode(string_list, device = device)\n",
        "        return embeddings\n",
        "\n",
        "    def model_token_length(self: LMRANK) -> int:\n",
        "\n",
        "        model = self.model\n",
        "\n",
        "        return model.max_seq_length\n",
        "\n",
        "    def get_keyphrases_embeddings(\n",
        "            self: LMRANK, candidate_keyphrases: List[Tuple[str, List[int]]]) -> numpy.typing.NDArray[Any]:\n",
        "\n",
        "        embeddings = self.encode([keyphrase for keyphrase, _ in candidate_keyphrases])\n",
        "        return embeddings\n",
        "\n",
        "    def get_document_embedding(self: LMRANK) -> numpy.typing.NDArray[np.float32]:\n",
        "\n",
        "        if len(self.doc) <= self.model_token_length():\n",
        "            document_embedding = self.encode(self.text)\n",
        "        else:\n",
        "            chunks = LMRANK.create_chunks(self.text, self.model_token_length())\n",
        "            document_embedding = np.mean(\n",
        "                self.encode(chunks), axis = 0\n",
        "            )\n",
        "\n",
        "        return document_embedding\n",
        "\n",
        "    def calculate_positional_scores(\n",
        "            self: LMRANK, candidate_keyphrases: List[Tuple[str, int]]\n",
        "        ) -> numpy.typing.NDArray[np.float32]:\n",
        "\n",
        "        scores = np.array([\n",
        "            1 / (position + 1)\n",
        "            for _, position in candidate_keyphrases\n",
        "        ])\n",
        "\n",
        "        e_scores = numpy.exp(scores - np.max(scores))\n",
        "        scores = e_scores / e_scores.sum(axis = 0)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def LMRank(\n",
        "            self: LMRANK, text: str, sentence_seps: str = '.?!',\n",
        "            deduplicate: bool = False, keep_nouns_adjs: bool = True, positional_feature: bool = True\n",
        "        ) -> List[Tuple[str, float]]:\n",
        "\n",
        "        candidate_keyphrases = self.extract_candidate_keyphrases(\n",
        "            text, sentence_seps, deduplicate, keep_nouns_adjs\n",
        "        )\n",
        "\n",
        "        if not candidate_keyphrases:\n",
        "            return []\n",
        "\n",
        "        embeddings = self.get_keyphrases_embeddings(candidate_keyphrases)\n",
        "        document_embedding = np.atleast_2d(self.get_document_embedding())\n",
        "\n",
        "        unranked_ids = np.array(range(len(embeddings))).astype(np.int64)\n",
        "\n",
        "        embedding_dim = len(embeddings[0])\n",
        "\n",
        "        index = faiss.index_factory(embedding_dim, 'IDMap,Flat', faiss.METRIC_INNER_PRODUCT)\n",
        "\n",
        "        faiss.normalize_L2(embeddings)\n",
        "\n",
        "        index.add_with_ids(embeddings, unranked_ids)\n",
        "\n",
        "        faiss.normalize_L2(document_embedding)\n",
        "\n",
        "        similarities, ranked_ids = index.search(document_embedding, len(candidate_keyphrases))\n",
        "\n",
        "        if positional_feature:\n",
        "            scores = self.calculate_positional_scores(candidate_keyphrases)\n",
        "\n",
        "            ranked_list = [\n",
        "                (candidate_keyphrases[key_id][0], float(sim * score))\n",
        "                for key_id, sim, score in zip(ranked_ids[0], similarities[0], scores)]\n",
        "\n",
        "        else:\n",
        "            ranked_list = [\n",
        "                (candidate_keyphrases[key_id][0], float(sim))\n",
        "                for key_id, sim in zip(ranked_ids[0], similarities[0])]\n",
        "\n",
        "        seen_stems = {}\n",
        "        for phrase, score in ranked_list:\n",
        "            tokens = [token.text for token in nlp(phrase)]\n",
        "            stems = [stemmer.stem(tok.lower()) for tok in tokens]\n",
        "            stemmed_phrase = \" \".join(stems)\n",
        "\n",
        "            if stemmed_phrase not in seen_stems:\n",
        "                seen_stems[stemmed_phrase] = score\n",
        "            elif score > seen_stems[stemmed_phrase]:\n",
        "                seen_stems[stemmed_phrase] = score\n",
        "\n",
        "        ranked_list = sorted(seen_stems.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return ranked_list"
      ],
      "metadata": {
        "id": "Kz0Iz0aGavqu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pke.unsupervised import *\n",
        "from timeit import default_timer as timer"
      ],
      "metadata": {
        "id": "9F6lvovVAzu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = {}\n",
        "outputs2 = {}\n",
        "elapsed_times = {}\n",
        "for model in [TF, TfIdf, KPMiner, YAKE, TextRank1, SingleRank, PositionRank, KeyBERT, MDERANK, LMRANK]:\n",
        "    outputs[model.__name__] = []\n",
        "    outputs2[model.__name__] = []\n",
        "\n",
        "    extractor = model()\n",
        "    start = timer()\n",
        "\n",
        "    if model.__name__ == \"KPMiner\":\n",
        "        for i, doc in enumerate(tqdm(all_data)):\n",
        "            extractor.load_document(input=doc, language='en')\n",
        "            extractor.candidate_selection(lasf=3, cutoff=400)\n",
        "\n",
        "            n_dynamic = len(references[i])\n",
        "\n",
        "            extractor.candidate_weighting(df=df)\n",
        "\n",
        "            outputs[model.__name__].append([u for u,v in extractor.get_n_best(n=n_dynamic, stemming=True)])\n",
        "            doc_candidates = dict(extractor.weights)\n",
        "            outputs2[model.__name__].append(doc_candidates)\n",
        "\n",
        "    elif model.__name__ == \"YAKE\":\n",
        "        for i, doc in enumerate(tqdm(all_data)):\n",
        "            extractor.load_document(input=doc, language='en')\n",
        "            extractor.candidate_selection(n=3)\n",
        "\n",
        "            n_dynamic = len(references[i])\n",
        "\n",
        "            use_stems = True\n",
        "            extractor.candidate_weighting(use_stems=use_stems)\n",
        "\n",
        "            outputs[model.__name__].append([u for u,v in extractor.get_n_best(n=n_dynamic, stemming=True)])\n",
        "            doc_candidates = dict(extractor.weights)\n",
        "            outputs2[model.__name__].append(doc_candidates)\n",
        "\n",
        "    elif model.__name__ == \"TextRank1\":\n",
        "        for i, doc in enumerate(tqdm(all_data)):\n",
        "            extractor.load_document(input=doc, language='en')\n",
        "\n",
        "            extractor.candidate_weighting(window=2, pos={'NOUN', 'PROPN', 'ADJ'}, top_percent=0.33)\n",
        "            outputs[model.__name__].append([u for u,v in extractor.get_n_best(n=n_dynamic, stemming=True)])\n",
        "            doc_candidates = dict(extractor.weights)\n",
        "            outputs2[model.__name__].append(doc_candidates)\n",
        "\n",
        "    elif model.__name__ == \"SingleRank\":\n",
        "        for i, doc in enumerate(tqdm(all_data)):\n",
        "            extractor.load_document(input=doc, language='en')\n",
        "            extractor.candidate_selection(pos={'NOUN', 'PROPN', 'ADJ'})\n",
        "\n",
        "            extractor.candidate_weighting(window=10, pos={'NOUN', 'PROPN', 'ADJ'})\n",
        "            outputs[model.__name__].append([u for u,v in extractor.get_n_best(n=n_dynamic, stemming=True)])\n",
        "            doc_candidates = dict(extractor.weights)\n",
        "            outputs2[model.__name__].append(doc_candidates)\n",
        "\n",
        "    elif model.__name__ == \"PositionRank\":\n",
        "        for i, doc in enumerate(tqdm(all_data)):\n",
        "            extractor.load_document(input=doc, language='en')\n",
        "            extractor.candidate_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\", maximum_word_number=3)\n",
        "\n",
        "            extractor.candidate_weighting(window=10, pos={'NOUN', 'PROPN', 'ADJ'})\n",
        "            outputs[model.__name__].append([u for u,v in extractor.get_n_best(n=n_dynamic, stemming=True)])\n",
        "            doc_candidates = dict(extractor.weights)\n",
        "            outputs2[model.__name__].append(doc_candidates)\n",
        "\n",
        "    elif model.__name__ == \"KeyBERT\":\n",
        "        for i, doc in enumerate(tqdm(all_data)):\n",
        "            extractor.load_document(input=doc, language='en')\n",
        "            extractor.grammar_selection(grammar=\"NP: {<ADJ>*<NOUN|PROPN>+}\")\n",
        "\n",
        "            n_dynamic = len(references[i])\n",
        "\n",
        "            extractor.candidate_weighting(input=doc)\n",
        "            outputs[model.__name__].append([u for u,v in extractor.get_n_best(n=n_dynamic, stemming=True)])\n",
        "            doc_candidates = dict(extractor.weights)\n",
        "            outputs2[model.__name__].append(doc_candidates)\n",
        "\n",
        "    elif model.__name__ == \"MDERANK\":\n",
        "\n",
        "        for i, doc in enumerate(tqdm(all_data)):\n",
        "            n_dynamic = len(references[i])\n",
        "\n",
        "            doc = str(doc)\n",
        "            dist_sorted = extractor.MDERank(text=doc)\n",
        "            outputs[model.__name__].append([u for u, v in dist_sorted[:n_dynamic]])\n",
        "            outputs2[model.__name__].append({u: v for u, v in dist_sorted})\n",
        "\n",
        "    elif model.__name__ == \"LMRANK\":\n",
        "\n",
        "        for i, text in enumerate(tqdm(all_data)):\n",
        "            n_dynamic = len(references[i])\n",
        "\n",
        "            text = str(text)\n",
        "            ranked_list = extractor.LMRank(text=text)\n",
        "\n",
        "            outputs[model.__name__].append([u for u, v in ranked_list[:n_dynamic]])\n",
        "            outputs2[model.__name__].append({u: v for u, v in ranked_list})\n",
        "\n",
        "    else:\n",
        "        for i, doc in enumerate(tqdm(all_data)):\n",
        "            extractor.load_document(input=doc, language='en')\n",
        "            extractor.candidate_selection(n=3)\n",
        "\n",
        "            n_dynamic = len(references[i])\n",
        "\n",
        "            if model.__name__ == \"TfIdf\":\n",
        "                extractor.candidate_weighting(df=df)\n",
        "            else:\n",
        "                extractor.candidate_weighting()\n",
        "            outputs[model.__name__].append([u for u,v in extractor.get_n_best(n=n_dynamic, stemming=True)])\n",
        "            doc_candidates = dict(extractor.weights)\n",
        "            outputs2[model.__name__].append(doc_candidates)\n",
        "\n",
        "    end = timer()\n",
        "    elapsed_times[model.__name__] = end - start\n",
        "\n",
        "    if model.__name__ == \"KeyBERT\":\n",
        "      for i, doc_candidates in enumerate(outputs[model.__name__]):\n",
        "          stemmed_candidates = []\n",
        "          for candidate in doc_candidates:\n",
        "              tokens = [token.text for token in nlp(candidate)]\n",
        "              stems = [stemmer.stem(tok.lower()) for tok in tokens]\n",
        "              stemmed_candidate = \" \".join(stems)\n",
        "              stemmed_candidates.append(stemmed_candidate)\n",
        "          outputs[model.__name__][i] = stemmed_candidates\n",
        "      for i, doc_phrases in enumerate(outputs2[model.__name__]):\n",
        "          stemmed_phrases = {}\n",
        "          for phrase, score in doc_phrases.items():\n",
        "              tokens = [token.text for token in nlp(phrase)]\n",
        "              stems = [stemmer.stem(tok.lower()) for tok in tokens]\n",
        "              stemmed_phrase = \" \".join(stems)\n",
        "              stemmed_phrases[stemmed_phrase] = score\n",
        "          outputs2[model.__name__][i] = stemmed_phrases"
      ],
      "metadata": {
        "id": "ZA3yXHmkwXrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_exact(top_N_candidates, references, cutoff=None):\n",
        "    if cutoff is None:\n",
        "        cutoff = len(references)\n",
        "    cutoff = min(cutoff, len(top_N_candidates))\n",
        "    P = len(set(top_N_candidates[:cutoff]) & set(references)) / len(top_N_candidates[:cutoff])\n",
        "    R = len(set(top_N_candidates[:cutoff]) & set(references)) / len(references)\n",
        "    F = (2 * P * R) / (P + R) if (P + R) > 0 else 0\n",
        "    return (P, R, F)\n",
        "\n",
        "def split_into_tokens(phrases):\n",
        "    tokens = set()\n",
        "    for phrase in phrases:\n",
        "        tokens.update(phrase.split())\n",
        "    return tokens\n",
        "\n",
        "def evaluate_partial(top_N_candidates, references, cutoff=None):\n",
        "    if cutoff is None:\n",
        "        cutoff = len(references)\n",
        "    cutoff = min(cutoff, len(top_N_candidates))\n",
        "\n",
        "    predicted_tokens = split_into_tokens(top_N_candidates[:cutoff])\n",
        "    reference_tokens = split_into_tokens(references)\n",
        "\n",
        "    intersection = len(predicted_tokens & reference_tokens)\n",
        "\n",
        "    P = intersection / len(predicted_tokens) if predicted_tokens else 0\n",
        "    R = intersection / len(reference_tokens) if reference_tokens else 0\n",
        "    F = (2 * P * R) / (P + R) if (P + R) > 0 else 0\n",
        "    return (P, R, F)\n",
        "\n",
        "def evaluate_harmonic(F1, pF1):\n",
        "    return (2 * F1 * pF1) / (F1 + pF1) if (F1 + pF1) > 0 else 0"
      ],
      "metadata": {
        "id": "dt29HszEuqmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "NZy48DoQHWo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"## Benchmarking on {}\".format(benchmark))\n",
        "print(\"| Model       | it/s |     F    |   pF    |    hF    |\")\n",
        "print(\"| :---------- | ----:| -------: | ------: | -------: |\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for model in outputs:\n",
        "    scores_exact = []\n",
        "    scores_partial = []\n",
        "    scores_harmonic = []\n",
        "\n",
        "    for i, output in enumerate(outputs[model]):\n",
        "        if not output:\n",
        "            P_exact, R_exact, F_exact = (0, 0, 0)\n",
        "            P_partial, R_partial, F_partial = (0, 0, 0)\n",
        "            hF1 = 0\n",
        "        else:\n",
        "            P_exact, R_exact, F_exact = evaluate_exact(output, references[i], cutoff=None)\n",
        "            P_partial, R_partial, F_partial = evaluate_partial(output, references[i], cutoff=None)\n",
        "            hF1 = evaluate_harmonic(F_exact, F_partial)\n",
        "\n",
        "        scores_exact.append((P_exact, R_exact, F_exact))\n",
        "        scores_partial.append((P_partial, R_partial, F_partial))\n",
        "        scores_harmonic.append(hF1)\n",
        "\n",
        "    P_exact, R_exact, F_exact = np.mean(scores_exact, axis=0)\n",
        "    P_partial, R_partial, F_partial = np.mean(scores_partial, axis=0)\n",
        "    hF1_mean = np.mean(scores_harmonic)\n",
        "\n",
        "    print(\"| {}  | {:.5f} | {:.5f} | {:.5f} | {:.5f} |\".format(\n",
        "        model,\n",
        "        len(all_data) / elapsed_times[model],\n",
        "        F_exact,\n",
        "        F_partial,\n",
        "        hF1_mean\n",
        "    ))\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": model,\n",
        "        \"F\": F_exact,\n",
        "        \"pF\": F_partial,\n",
        "        \"hF\": hF1_mean\n",
        "    })"
      ],
      "metadata": {
        "id": "mRvk8dt4wbjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "GDm9aCS5A_uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "eyLPPZypuvl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results"
      ],
      "metadata": {
        "id": "bKbsGIuXwd8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "1tQWfWfsBCJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"KDD_results.pkl\", \"wb\") as f:\n",
        "    pickle.dump(df_results, f)"
      ],
      "metadata": {
        "id": "-7Zrr_x3zK3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats"
      ],
      "metadata": {
        "id": "0nYrb_XmBD_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_outputs(outputs2):\n",
        "    outputs3 = {}\n",
        "\n",
        "    for method, documents in outputs2.items():\n",
        "        normalized_documents = []\n",
        "\n",
        "        for doc in documents:\n",
        "            if not doc:\n",
        "                normalized_documents.append({})\n",
        "                continue\n",
        "\n",
        "            keys = list(doc.keys())\n",
        "            values = list(doc.values())\n",
        "\n",
        "            ranks = scipy.stats.rankdata(values, method=\"dense\")\n",
        "            rank_dict = dict(zip(keys, ranks))\n",
        "\n",
        "            x_min = min(ranks)\n",
        "            x_max = max(ranks)\n",
        "\n",
        "            if x_max == x_min:\n",
        "                norm_doc = {k: 0.0 for k in doc}\n",
        "\n",
        "            elif method in [\"TF\", \"TfIdf\", \"KPMiner\", \"TextRank1\", \"SingleRank\", \"PositionRank\", \"KeyBERT\", \"LMRANK\"]:\n",
        "                norm_doc = {k: float((rank_dict[k] - x_min) / (x_max - x_min)) for k in doc}\n",
        "\n",
        "            elif method in [\"YAKE\", \"MDERANK\"]:\n",
        "                norm_doc = {k: float((x_max - rank_dict[k]) / (x_max - x_min)) for k in doc}\n",
        "\n",
        "            normalized_documents.append(norm_doc)\n",
        "\n",
        "        outputs3[method] = normalized_documents\n",
        "\n",
        "    return outputs3"
      ],
      "metadata": {
        "id": "fZSMK2yo0bSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs3 = normalize_outputs(outputs2)"
      ],
      "metadata": {
        "id": "7VSYZx-p0m0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "HWlBJq11BJ9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_DTM(outputs, outputs3, method, zeta=0.5, top_k=50):\n",
        "\n",
        "    docs_phrases = outputs[method]\n",
        "    docs_scores = outputs3[method]\n",
        "\n",
        "    k_score = round(top_k * zeta)\n",
        "    k_freq = top_k - k_score\n",
        "\n",
        "    score_sum = defaultdict(float)\n",
        "    for doc_scores in docs_scores:\n",
        "        for phrase, score in doc_scores.items():\n",
        "            score_sum[phrase] += score\n",
        "    top_score_ranked = sorted(score_sum.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    freq_counter = Counter()\n",
        "    for phrases in docs_phrases:\n",
        "        freq_counter.update(phrases)\n",
        "    most_common = freq_counter.most_common()\n",
        "\n",
        "    freq_terms = []\n",
        "    last_freq = None\n",
        "    for i, (phrase, freq) in enumerate(most_common):\n",
        "        if i < k_freq:\n",
        "            freq_terms.append((phrase, freq))\n",
        "            last_freq = freq\n",
        "        elif freq == last_freq:\n",
        "            freq_terms.append((phrase, freq))\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    if len(freq_terms) > k_freq:\n",
        "        target_freq = freq_terms[k_freq - 1][1]\n",
        "        tie_start_idx = next(i for i, (_, freq) in enumerate(freq_terms) if freq == target_freq)\n",
        "        tied_candidates = [x for x in freq_terms[tie_start_idx:] if x[1] == target_freq]\n",
        "\n",
        "        tied_candidates_sorted = sorted(\n",
        "            tied_candidates,\n",
        "            key=lambda x: score_sum.get(x[0], 0),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        num_needed = k_freq - tie_start_idx\n",
        "        freq_terms = freq_terms[:tie_start_idx] + tied_candidates_sorted[:num_needed]\n",
        "\n",
        "    top_freq_terms = [phrase for phrase, _ in freq_terms]\n",
        "\n",
        "    selected_terms = set(top_freq_terms)\n",
        "    final_phrases = list(top_freq_terms)\n",
        "\n",
        "    for phrase, _ in top_score_ranked:\n",
        "        if phrase not in selected_terms:\n",
        "            final_phrases.append(phrase)\n",
        "            selected_terms.add(phrase)\n",
        "        if len(final_phrases) == top_k:\n",
        "            break\n",
        "\n",
        "    matrix = []\n",
        "    for doc_scores in docs_scores:\n",
        "        row = [doc_scores.get(term, 0) for term in final_phrases]\n",
        "        matrix.append(row)\n",
        "\n",
        "    df_matrix = pd.DataFrame(matrix, columns=final_phrases)\n",
        "    return df_matrix"
      ],
      "metadata": {
        "id": "cvMqhD7Q1AvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_all_dtms(outputs, outputs3, dataset_name, method_list, zeta_list=[0.25, 0.5, 0.75], top_k=50):\n",
        "    dtm_dict = {}\n",
        "\n",
        "    for method in method_list:\n",
        "        for zeta in zeta_list:\n",
        "            df = build_DTM(outputs, outputs3, method, zeta=zeta, top_k=top_k)\n",
        "\n",
        "            zeta_str = str(zeta).replace('.', '_')\n",
        "            var_name = f\"{dataset_name}_{method}_zeta{zeta_str}\"\n",
        "\n",
        "            dtm_dict[var_name] = df\n",
        "\n",
        "    with open(f\"{dataset_name}_dtm_dict.pkl\", \"wb\") as f:\n",
        "        pickle.dump(dtm_dict, f)\n",
        "\n",
        "    return dtm_dict"
      ],
      "metadata": {
        "id": "7uE0DQSM1jyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"KDD\"\n",
        "method_list = [\"TF\", \"TfIdf\", \"KPMiner\", \"YAKE\", \"TextRank1\", \"SingleRank\", \"PositionRank\", \"KeyBERT\", \"MDERANK\", \"LMRANK\"]\n",
        "dtms = save_all_dtms(outputs, outputs3, dataset_name, method_list)"
      ],
      "metadata": {
        "id": "3OhCtVwq2ugP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"KDD_dtm_dict.pkl\", \"rb\") as f:\n",
        "    DTM_dict = pickle.load(f)"
      ],
      "metadata": {
        "id": "fo1UkRAS2vbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DTM_dict.keys())"
      ],
      "metadata": {
        "id": "VA3bkbR3wiDx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}