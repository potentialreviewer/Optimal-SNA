{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VdaOZ_a3opK"
      },
      "outputs": [],
      "source": [
        "!pip install POT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import urllib.request\n",
        "import networkx as nx\n",
        "import ot"
      ],
      "metadata": {
        "id": "HaxYnK4t4RKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/potentialreviewer/Optimal-SNA/main/data/500N-KP-Crowd_dtm_dict.pkl\"\n",
        "file_name = \"500N-KP-Crowd_dtm_dict.pkl\"\n",
        "\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "with open(\"500N-KP-Crowd_dtm_dict.pkl\", \"rb\") as f:\n",
        "    DTM_dict = pickle.load(f)"
      ],
      "metadata": {
        "id": "ES6Soco9M168"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TTMBuilder:\n",
        "    def __init__(self, dtm_dict, dataset_name, method, zeta):\n",
        "        self.dtm_dict = dtm_dict\n",
        "        self.dataset_name = dataset_name\n",
        "        self.method = method\n",
        "        self.zeta = zeta\n",
        "\n",
        "        self.key = f\"{dataset_name}_{method}_zeta{zeta}\"\n",
        "\n",
        "        self.dtm = dtm_dict[self.key]\n",
        "        self.ttm = None\n",
        "\n",
        "    def build_CF_ttm(self):\n",
        "        binary_dtm = (self.dtm > 0).astype(int)\n",
        "        cf_matrix = binary_dtm.T.dot(binary_dtm)\n",
        "\n",
        "        self.ttm = cf_matrix\n",
        "        return self.ttm\n",
        "\n",
        "    def build_Dice_ttm(self):\n",
        "        cf_matrix = self.build_CF_ttm()\n",
        "        diag = np.diag(cf_matrix)\n",
        "\n",
        "        i_diag = diag[:, np.newaxis]\n",
        "        j_diag = diag[np.newaxis, :]\n",
        "        dice_matrix = 2 * cf_matrix / (i_diag + j_diag)\n",
        "        np.fill_diagonal(dice_matrix.values, 1.0)\n",
        "\n",
        "        self.ttm = pd.DataFrame(dice_matrix, index=cf_matrix.index, columns=cf_matrix.columns)\n",
        "        return self.ttm\n",
        "\n",
        "    def build_Jaccard_ttm(self):\n",
        "        cf_matrix = self.build_CF_ttm()\n",
        "        diag = np.diag(cf_matrix)\n",
        "\n",
        "        i_diag = diag[:, np.newaxis]\n",
        "        j_diag = diag[np.newaxis, :]\n",
        "        denominator = i_diag + j_diag - cf_matrix\n",
        "        jaccard_matrix = cf_matrix / denominator\n",
        "        np.fill_diagonal(jaccard_matrix.values, 1.0)\n",
        "\n",
        "        self.ttm = pd.DataFrame(jaccard_matrix, index=cf_matrix.index, columns=cf_matrix.columns)\n",
        "        return self.ttm\n",
        "\n",
        "    def build_Cosine_ttm(self):\n",
        "        term_vectors = self.dtm.T\n",
        "        similarity_matrix = term_vectors.dot(term_vectors.T)\n",
        "        norms = (term_vectors ** 2).sum(axis=1) ** 0.5\n",
        "        self.ttm = similarity_matrix.divide(norms, axis=0).divide(norms, axis=1)\n",
        "        return self.ttm\n",
        "\n",
        "    def build_Euclidean_ttm(self):\n",
        "        term_vectors = self.dtm.T.values\n",
        "        term_names = self.dtm.columns\n",
        "\n",
        "        dist_matrix = np.zeros((len(term_vectors), len(term_vectors)))\n",
        "        for i in range(len(term_vectors)):\n",
        "            for j in range(i + 1, len(term_vectors)):\n",
        "                dist = np.sqrt(np.sum((term_vectors[i] - term_vectors[j]) ** 2))\n",
        "                dist_matrix[i, j] = dist\n",
        "                dist_matrix[j, i] = dist\n",
        "\n",
        "        max_dist = np.max(dist_matrix)\n",
        "        similarity_matrix = 1 - (dist_matrix / max_dist)\n",
        "        self.ttm = pd.DataFrame(similarity_matrix, index=term_names, columns=term_names)\n",
        "        return self.ttm\n",
        "\n",
        "    def build_Manhattan_ttm(self):\n",
        "        term_vectors = self.dtm.T.values\n",
        "        term_names = self.dtm.columns\n",
        "\n",
        "        dist_matrix = np.zeros((len(term_vectors), len(term_vectors)))\n",
        "        for i in range(len(term_vectors)):\n",
        "            for j in range(i + 1, len(term_vectors)):\n",
        "                dist = np.sum(np.abs(term_vectors[i] - term_vectors[j]))\n",
        "                dist_matrix[i, j] = dist\n",
        "                dist_matrix[j, i] = dist\n",
        "\n",
        "        max_dist = np.max(dist_matrix)\n",
        "        similarity_matrix = 1 - (dist_matrix / max_dist)\n",
        "        self.ttm = pd.DataFrame(similarity_matrix, index=term_names, columns=term_names)\n",
        "        return self.ttm\n",
        "\n",
        "    def build_DMD_ttm(self):\n",
        "        dtm = self.dtm.values\n",
        "        m, n = dtm.shape\n",
        "\n",
        "        cost_matrix = np.zeros((m, m))\n",
        "        for k in range(m):\n",
        "            for l in range(k + 1, m):\n",
        "                dist = np.sqrt(np.sum((dtm[k] - dtm[l]) ** 2))\n",
        "                cost_matrix[k, l] = dist\n",
        "                cost_matrix[l, k] = dist\n",
        "\n",
        "        term_vectors = dtm.T\n",
        "\n",
        "        dmd_matrix = np.full((n, n), np.nan)\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                p = term_vectors[i]\n",
        "                q = term_vectors[j]\n",
        "\n",
        "                if np.sum(p) == 0 or np.sum(q) == 0:\n",
        "                    continue\n",
        "                else:\n",
        "                    p_norm = p / np.sum(p)\n",
        "                    q_norm = q / np.sum(q)\n",
        "                    dmd = ot.emd2(p_norm, q_norm, cost_matrix)\n",
        "\n",
        "                    dmd_matrix[i, j] = dmd\n",
        "                    dmd_matrix[j, i] = dmd\n",
        "\n",
        "        finite_mask = ~np.isnan(dmd_matrix)\n",
        "        max_dist = np.max(dmd_matrix[finite_mask])\n",
        "        dmd_matrix[np.isnan(dmd_matrix)] = max_dist\n",
        "\n",
        "        sim_matrix = 1 - (dmd_matrix / max_dist)\n",
        "\n",
        "        self.ttm = pd.DataFrame(sim_matrix, index=self.dtm.columns, columns=self.dtm.columns)\n",
        "\n",
        "        return self.ttm"
      ],
      "metadata": {
        "id": "ykHOjAULdyQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ttm_dict(dtm_dict, edge_methods):\n",
        "\n",
        "    ttm_dict = {}\n",
        "\n",
        "    for key in dtm_dict:\n",
        "        prefix, zeta = key.split('_zeta')\n",
        "        dataset, method = prefix.split('_')\n",
        "\n",
        "        builder = TTMBuilder(dtm_dict, dataset, method, zeta)\n",
        "\n",
        "        if dataset not in ttm_dict:\n",
        "            ttm_dict[dataset] = {}\n",
        "        if method not in ttm_dict[dataset]:\n",
        "            ttm_dict[dataset][method] = {}\n",
        "        if zeta not in ttm_dict[dataset][method]:\n",
        "            ttm_dict[dataset][method][zeta] = {}\n",
        "\n",
        "        for em in edge_methods:\n",
        "            func = getattr(builder, f\"build_{em}_ttm\")\n",
        "            ttm_dict[dataset][method][zeta][em] = func()\n",
        "\n",
        "    return ttm_dict"
      ],
      "metadata": {
        "id": "Ii6QQruwdyUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_methods = [\"CF\", \"Dice\", \"Jaccard\", \"Cosine\", \"Euclidean\", \"Manhattan\", \"DMD\"]\n",
        "TTM_dict = generate_ttm_dict(DTM_dict, edge_methods)"
      ],
      "metadata": {
        "id": "eTLN088HdqvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"500N-KP-Crowd_ttm_dict.pkl\", \"wb\") as f:\n",
        "    pickle.dump(TTM_dict, f)"
      ],
      "metadata": {
        "id": "vikN4g5MVHDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"500N-KP-Crowd_ttm_dict.pkl\", \"rb\") as f:\n",
        "    TTM_dict = pickle.load(f)"
      ],
      "metadata": {
        "id": "5AQwmR3BdT8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_graph_from_ttm(ttm_df, omega=0.1):\n",
        "\n",
        "    G = nx.Graph()\n",
        "    terms = ttm_df.index.tolist()\n",
        "    G.add_nodes_from(terms)\n",
        "\n",
        "    edges = []\n",
        "    for i in range(1, len(terms)):\n",
        "        for j in range(0, i):\n",
        "            weight = ttm_df.iat[i, j]\n",
        "            if weight > 0:\n",
        "                edges.append((terms[i], terms[j], weight))\n",
        "\n",
        "    edges.sort(key=lambda x: x[2], reverse=True)\n",
        "    k = int(len(edges) * omega)\n",
        "    top_edges = edges[:k]\n",
        "\n",
        "    for u, v, w in top_edges:\n",
        "        G.add_edge(u, v, weight=w)\n",
        "\n",
        "    return G"
      ],
      "metadata": {
        "id": "QV8glPcrdT_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_graph_dict_from_ttm_dict(ttm_dict, omega=0.1):\n",
        "\n",
        "    graph_dict = {}\n",
        "\n",
        "    for dataset in ttm_dict:\n",
        "        graph_dict[dataset] = {}\n",
        "        for method in ttm_dict[dataset]:\n",
        "            graph_dict[dataset][method] = {}\n",
        "            for zeta in ttm_dict[dataset][method]:\n",
        "                graph_dict[dataset][method][zeta] = {}\n",
        "                for edge_method in ttm_dict[dataset][method][zeta]:\n",
        "                    ttm = ttm_dict[dataset][method][zeta][edge_method]\n",
        "                    G = build_graph_from_ttm(ttm, omega=omega)\n",
        "                    graph_dict[dataset][method][zeta][edge_method] = G\n",
        "\n",
        "    return graph_dict"
      ],
      "metadata": {
        "id": "6HFiZG7ZdUCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Graph_dict = generate_graph_dict_from_ttm_dict(TTM_dict, omega=0.1)"
      ],
      "metadata": {
        "id": "-lBFj7_9cLj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"500N-KP-Crowd_graph_dict.pkl\", \"wb\") as f:\n",
        "    pickle.dump(Graph_dict, f)"
      ],
      "metadata": {
        "id": "4_rZma1McLm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"500N-KP-Crowd_graph_dict.pkl\", \"rb\") as f:\n",
        "    graph_dict = pickle.load(f)"
      ],
      "metadata": {
        "id": "ssJSWT_GcLp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_empirical_fc(G, runs=100, seed=2025):\n",
        "    random.seed(seed)\n",
        "    V = set(G.nodes())\n",
        "    V_size = len(V)\n",
        "\n",
        "    GC_nodes = set(max(nx.connected_components(G), key=len))\n",
        "    GC_size = len(GC_nodes)\n",
        "    P0 = GC_size / V_size\n",
        "\n",
        "    fc_list = []\n",
        "    all_traces= []\n",
        "\n",
        "    for _ in range(runs):\n",
        "        G_simulation = G.copy()\n",
        "        node_list = list(G_simulation.nodes())\n",
        "        random.shuffle(node_list)\n",
        "\n",
        "        removed_nodes = set()\n",
        "        trace = []\n",
        "\n",
        "        fc_recorded = False\n",
        "\n",
        "        for step, node in enumerate(node_list):\n",
        "            G_simulation.remove_node(node)\n",
        "            removed_nodes.add(node)\n",
        "\n",
        "            remaining_GC_nodes = GC_nodes - removed_nodes\n",
        "\n",
        "            if not remaining_GC_nodes:\n",
        "                Pf = 0\n",
        "            else:\n",
        "                subgraph = G_simulation.subgraph(remaining_GC_nodes)\n",
        "                cc_list = list(nx.connected_components(subgraph))\n",
        "                if not cc_list:\n",
        "                    Pf = 0\n",
        "                else:\n",
        "                    GC_now = max(cc_list, key=len)\n",
        "                    Pf = len(GC_now) / V_size\n",
        "\n",
        "                    largest_cc = max(nx.connected_components(G_simulation), key=len)\n",
        "                    if len(largest_cc) > len(GC_now):\n",
        "                        Pf = 0\n",
        "\n",
        "            f = (step + 1) / V_size\n",
        "            trace.append((f, Pf / P0))\n",
        "\n",
        "            if not fc_recorded and (Pf / P0 == 0):\n",
        "                fc_list.append(f)\n",
        "                fc_recorded = True\n",
        "\n",
        "        all_traces.append(trace)\n",
        "\n",
        "    return np.mean(fc_list), all_traces"
      ],
      "metadata": {
        "id": "62Y5fkPAcYBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ratio(G):\n",
        "    degrees = [d for _, d in G.degree()]\n",
        "    k_avg = np.mean(degrees)\n",
        "    k2_avg = np.mean(np.square(degrees))\n",
        "\n",
        "    ratio = k2_avg / k_avg\n",
        "\n",
        "    return ratio"
      ],
      "metadata": {
        "id": "IA6jtyo-cLtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ri(fc_empirical, fc_theoretical):\n",
        "    if fc_empirical > fc_theoretical:\n",
        "        return (fc_empirical - fc_theoretical) / (1 - fc_theoretical)\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "FoSusKZScLy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_graph_dict(graph_dict, dataset_name, runs=100, seed=2025):\n",
        "    records = []\n",
        "\n",
        "    for method in graph_dict[dataset_name]:\n",
        "        for zeta in graph_dict[dataset_name][method]:\n",
        "            for edge_method, G in graph_dict[dataset_name][method][zeta].items():\n",
        "                ratio = calculate_ratio(G)\n",
        "\n",
        "                num_isolated_nodes = len(list(nx.isolates(G)))\n",
        "                num_edges = G.number_of_edges()\n",
        "\n",
        "                if ratio > 2:\n",
        "                    fc_theoretical = 1 - 1 / (ratio - 1)\n",
        "                    fc_empirical, all_traces = calculate_empirical_fc(G, runs=runs, seed=seed)\n",
        "                    ri = calculate_ri(fc_empirical, fc_theoretical)\n",
        "                else:\n",
        "                    fc_theoretical = 0\n",
        "                    fc_empirical = 0\n",
        "                    ri = 0\n",
        "                    all_traces = []\n",
        "\n",
        "                records.append({\n",
        "                    'Dataset': dataset_name,\n",
        "                    'AKE Method': method,\n",
        "                    'Zeta': zeta,\n",
        "                    'Edge Measure': edge_method,\n",
        "                    'Molloy-Reed Criterion': ratio,\n",
        "                    'f_c (theoretical)': fc_theoretical,\n",
        "                    'f_c (empirical)': fc_empirical,\n",
        "                    'RI': ri,\n",
        "                    'Isolated Nodes': num_isolated_nodes,\n",
        "                    'Edge Count': num_edges,\n",
        "                    'All Traces': all_traces\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    return df"
      ],
      "metadata": {
        "id": "0H1w9wf7Z_3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = evaluate_graph_dict(graph_dict, '500N-KP-Crowd', runs=100, seed=2025)"
      ],
      "metadata": {
        "id": "S53taduLZ_7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "BoEHR4DPZ_9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"500N-KP-Crowd_percolation_results.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results_df, f)"
      ],
      "metadata": {
        "id": "zYZwnJGyaALy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}